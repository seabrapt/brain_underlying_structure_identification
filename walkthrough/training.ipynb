{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Packages (pip freeze) in requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import helper functions to generate graphs and timeseries\n",
    "from helper_functions import *\n",
    "\n",
    "# keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "# z normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# misc\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN (Our approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fnn(X_train,y_train, n_features, log_name=''):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "    csv_logger = CSVLogger(f'./logs/training_ffnn_{log_name}.log', separator=',', append=False)\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(n_features,)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, verbose=0, callbacks=[cb, csv_logger])\n",
    "    return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN [S. Machado et al.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cnn(X_train,y_train, n_features):\n",
    "    \"\"\"\n",
    "        Function to train a cnn model\n",
    "            Model architecture follows the structure used in [S. Machado et al.]\n",
    "    \"\"\"\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2, activation='relu', input_shape=(n_features,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # train\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, verbose=0, callbacks=[cb])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Identifiability Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_idgap(pred, y, dis_mpred, con_mpred):\n",
    "    \"\"\"\n",
    "        Calculate the accuracy of the model and the identifiability gap\n",
    "\n",
    "        accuracy:               measures the number of correct predictions\n",
    "        identifiability gap:    distance between the lowest predicted connected pair and \n",
    "                                the highest predicted disconnected pair\n",
    "                            \n",
    "        input:\n",
    "            pred:   predictions \n",
    "            y:      target\n",
    "            dis_mpred: disconnected preds\n",
    "            con_mpred: connected preds\n",
    "\n",
    "        output:\n",
    "            (accuracy, identifiability gap)\n",
    "\n",
    "    \"\"\"\n",
    "    # - - - testing \n",
    "    #Divide the connected and disconnected pairs values\n",
    "    test_size = len(y)\n",
    "    idd = y < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    idc = y > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    nc = np.sum(idc)\n",
    "\n",
    "    # predict and Normalize the values\n",
    "    true = y - 1\n",
    "\n",
    "    #Initialize the structures to save the data\n",
    "    con = np.zeros((nc))\n",
    "    dis = np.zeros((test_size-nc))\n",
    "\n",
    "    #Split the values belonging to connected and disconnected pairs\n",
    "    c2,c3 = 0,0\n",
    "    for i in range(len(true)):\n",
    "        if(true[i]>0):\n",
    "            con[c2] = pred[i]\n",
    "            c2+=1\n",
    "        else:\n",
    "            dis[c3] = pred[i]\n",
    "            c3+=1\n",
    "\n",
    "    #Threshold\n",
    "    mint = np.max(dis)\n",
    "    maxt = np.min(con)\n",
    "\n",
    "    #Compute metrics\n",
    "    idgap = maxt - ((mint+maxt)/2)\n",
    "\n",
    "    #Predicts the weight of the connection and classifies the data\n",
    "    truec_nn = np.sum(dis_mpred<1.5)\n",
    "    trued_nn = np.sum(con_mpred>1.5)\n",
    "\n",
    "    tall = truec_nn + trued_nn\n",
    "\n",
    "    acc = tall/(len(dis_mpred)+len(con_mpred))*100\n",
    "\n",
    "\n",
    "    #If the id gap is negative then we cant correctly classify the pairs as disconnected or connected\n",
    "    if(idgap < 0):\n",
    "        idgap=0\n",
    "    \n",
    "    return acc, idgap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph parameters\n",
    "undirected = True\n",
    "p = 0.5  \n",
    "sz = 40\n",
    "ss = 0\n",
    "se = 20\n",
    "\n",
    "# timeseries parameters\n",
    "tsize = 100000\n",
    "x0 = 0\n",
    "\n",
    "# noise values\n",
    "beta = 2\n",
    "alpha = 1\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "# get adjacency (adj) and interaction (A) matrices\n",
    "adj = get_adjacency(sz,p,undirected)\n",
    "A = get_A(adj,c,rho)\n",
    "\n",
    "# generate noise\n",
    "noise = generate_noise(sz, tsize, alpha, beta)\n",
    "\n",
    "# generate timeseries\n",
    "x = generate_timeseries(A,tsize,x0,noise)\n",
    "\n",
    "# partial observability\n",
    "x = x[:, ss:se]\n",
    "A = A[ss:se, ss:se]\n",
    "sz = se - ss\n",
    "\n",
    "# generate features\n",
    "features_scaled = get_inverted_features(x, 100)\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(features_scaled)\n",
    "y_test = get_target(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5 11 16 22 27 33 38 44 50]\n",
      "[  20000  120000  240000  340000  460000  560000  680000  780000  900000\n",
      " 1020000]\n"
     ]
    }
   ],
   "source": [
    "# graph parameters\n",
    "undirected = True\n",
    "# define the range of noise variance\n",
    "alpha = 1\n",
    "x0 = 0            \n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "# training parameters\n",
    "n_datasets = 10\n",
    "n_runs = 10     # pick the best performing model on the test set over 10 models \n",
    "\n",
    "\n",
    "# training is done with multiple datasets with varying correlated noise level (beta), \n",
    "# number of samples (tsize) and number of nodes (sz) \n",
    "betas = np.linspace(0,50,n_datasets).astype(int)\n",
    "offset = 20000\n",
    "tssizes = (betas+1)*offset\n",
    "print(betas)\n",
    "print(tssizes)\n",
    "\n",
    "n_datasets = tssizes.shape[0]\n",
    "\n",
    "# save models to pick best\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6060c2c914cf489c9ecd734df4ae1f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterate over number of runs\n",
    "for run in (t := trange(n_runs)):\n",
    "\n",
    "    # concatenate datasets for training \n",
    "    for i in range(n_datasets):\n",
    "\n",
    "        # random graph size (number of nodes)\n",
    "        sz = np.random.randint(30, 80) \n",
    "\n",
    "        # generate the adjacency and A matrices\n",
    "        adj = get_adjacency(sz,p,undirected)\n",
    "        A = get_A(adj,c,rho)\n",
    "        y = get_target(A)\n",
    "\n",
    "        # generate noise \n",
    "        noise = generate_noise(sz, tssizes[i], alpha, betas[i])\n",
    "\n",
    "        # generates the synthetic time series\n",
    "        x = generate_timeseries(A,tssizes[i],x0,noise)\n",
    "\n",
    "        # features\n",
    "        features_scaled = get_inverted_features(x, 100)\n",
    "        scaler = StandardScaler()\n",
    "        features_ss = scaler.fit_transform(features_scaled)\n",
    "\n",
    "        if i == 0:\n",
    "            ys = y\n",
    "            features_ss2 = features_ss\n",
    "\n",
    "        else:\n",
    "            ys = np.concatenate((ys,y),axis=0)\n",
    "            features_ss2 = np.concatenate((features_ss2, features_ss), axis=0)\n",
    "\n",
    "    # shuffle datasets\n",
    "    n_samples = features_ss2.shape[0]\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    features_ss2 = features_ss2[perm,:]\n",
    "    ys = ys[perm,:]\n",
    "\n",
    "    # train\n",
    "    model = train_model_fnn(features_ss2, ys, features_ss2.shape[1], log_name=f'_s_{run}')\n",
    "\n",
    "    # test\n",
    "    idd = y_test < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    idc = y_test > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    # calculate accuracy and idgap on test set\n",
    "    pred = model.predict(X_test, verbose=0)\n",
    "    dis_mpred = model.predict(X_test[idd,:], verbose=0)\n",
    "    con_mpred = model.predict(X_test[idc,:], verbose=0)\n",
    "    acc, idgap = get_acc_idgap(pred, y_test, dis_mpred, con_mpred)\n",
    "\n",
    "    # save model and performance\n",
    "    models.append((model, acc, idgap))\n",
    "\n",
    "    t.set_description(f'acc {acc} idgap {idgap}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.engine.sequential.Sequential object at 0x0000026D1AC78E20>, 100.0, 0.23927825689315796)\n",
      "[(<keras.engine.sequential.Sequential object at 0x0000026D1AC78E20>, 100.0, 0.23927825689315796)]\n"
     ]
    }
   ],
   "source": [
    "def sort_models(models):\n",
    "    \"\"\"\n",
    "        sort models by accuracy\n",
    "        in tiebreak:\n",
    "            sort models by the maximum of idgap\n",
    "    \"\"\"\n",
    "    return sorted(models, key=lambda x: ( -x[1], abs(0.5 - x[2]) ) )\n",
    "\n",
    "file_name = \"model_ffnn\"\n",
    "\n",
    "# sort models based on accuracy and idgap\n",
    "models = sort_models(models)\n",
    "_model = models[0]\n",
    "model = _model[0]\n",
    "print(_model)\n",
    "print(models)\n",
    "\n",
    "# saving model\n",
    "model.save_weights('trained_model.h5')\n",
    "\n",
    "# model = load_weights('trained_model.h5')  # loading model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
