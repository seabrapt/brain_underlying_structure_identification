{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Packages (pip freeze) in requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import helper functions to generate graphs and timeseries\n",
    "from helper_functions import *\n",
    "\n",
    "# keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "# z normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# misc\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN (Our approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fnn(X_train,y_train, n_features):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(n_features,)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, verbose=0, callbacks=[cb])\n",
    "    return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN [S. Machado et al.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cnn(X_train,y_train, n_features):\n",
    "    \"\"\"\n",
    "        Function to train a cnn model\n",
    "            Model architecture follows the structure used in [S. Machado et al.]\n",
    "    \"\"\"\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2, activation='relu', input_shape=(n_features,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # train\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, verbose=0, callbacks=[cb])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Identifiability Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_idgap(pred, y, dis_mpred, con_mpred):\n",
    "    \"\"\"\n",
    "        Calculate the accuracy of the model and the identifiability gap\n",
    "\n",
    "        accuracy:               measures the number of correct predictions\n",
    "        identifiability gap:    distance between the lowest predicted connected pair and \n",
    "                                the highest predicted disconnected pair\n",
    "                            \n",
    "        input:\n",
    "            pred:   predictions \n",
    "            y:      target\n",
    "            dis_mpred: disconnected preds\n",
    "            con_mpred: connected preds\n",
    "\n",
    "        output:\n",
    "            (accuracy, identifiability gap)\n",
    "\n",
    "    \"\"\"\n",
    "    # - - - testing \n",
    "    #Divide the connected and disconnected pairs values\n",
    "    test_size = len(y)\n",
    "    idd = y < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    idc = y > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    nc = np.sum(idc)\n",
    "\n",
    "    # predict and Normalize the values\n",
    "    true = y - 1\n",
    "\n",
    "    #Initialize the structures to save the data\n",
    "    con = np.zeros((nc))\n",
    "    dis = np.zeros((test_size-nc))\n",
    "\n",
    "    #Split the values belonging to connected and disconnected pairs\n",
    "    c2,c3 = 0,0\n",
    "    for i in range(len(true)):\n",
    "        if(true[i]>0):\n",
    "            con[c2] = pred[i]\n",
    "            c2+=1\n",
    "        else:\n",
    "            dis[c3] = pred[i]\n",
    "            c3+=1\n",
    "\n",
    "    #Threshold\n",
    "    mint = np.max(dis)\n",
    "    maxt = np.min(con)\n",
    "\n",
    "    #Compute metrics\n",
    "    idgap = maxt - ((mint+maxt)/2)\n",
    "\n",
    "    #Predicts the weight of the connection and classifies the data\n",
    "    truec_nn = np.sum(dis_mpred<1.5)\n",
    "    trued_nn = np.sum(con_mpred>1.5)\n",
    "\n",
    "    tall = truec_nn + trued_nn\n",
    "\n",
    "    acc = tall/(len(dis_mpred)+len(con_mpred))*100\n",
    "\n",
    "\n",
    "    #If the id gap is negative then we cant correctly classify the pairs as disconnected or connected\n",
    "    if(idgap < 0):\n",
    "        idgap=0\n",
    "    \n",
    "    return acc, idgap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph parameters\n",
    "undirected = True\n",
    "p = 0.5  \n",
    "sz = 40\n",
    "ss = 0\n",
    "se = 20\n",
    "\n",
    "# timeseries parameters\n",
    "tsize = 100000\n",
    "x0 = 0\n",
    "\n",
    "# noise values\n",
    "beta = 2\n",
    "alpha = 1\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "# get adjacency (adj) and interaction (A) matrices\n",
    "adj = get_adjacency(sz,p,undirected)\n",
    "A = get_A(adj,c,rho)\n",
    "\n",
    "# generate noise\n",
    "noise = generate_noise(sz, tsize, alpha, beta)\n",
    "\n",
    "# generate timeseries\n",
    "x = generate_timeseries(A,tsize,x0,noise)\n",
    "\n",
    "# partial observability\n",
    "x = x[:, ss:se]\n",
    "A = A[ss:se, ss:se]\n",
    "sz = se - ss\n",
    "\n",
    "# generate features\n",
    "features_scaled = get_inverted_features(x, 100)\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(features_scaled)\n",
    "y_test = get_target(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  5  7 10 13 15 18 21 23 26 28 31 34 36 39 42 44 47 50]\n",
      "[  20000   60000  120000  160000  220000  280000  320000  380000  440000\n",
      "  480000  540000  580000  640000  700000  740000  800000  860000  900000\n",
      "  960000 1020000]\n"
     ]
    }
   ],
   "source": [
    "# graph parameters\n",
    "undirected = True\n",
    "\n",
    "# define the range of noise variance\n",
    "alpha = 1\n",
    "x0 = 0            \n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "# training parameters\n",
    "n_datasets = 20\n",
    "n_runs = 10         # pick the best performing model on the test set over 10 models \n",
    "\n",
    "\n",
    "# training is done with multiple datasets with varying correlated noise level (beta), \n",
    "# number of samples (tsize) and number of nodes (sz) \n",
    "betas = np.linspace(0,50,n_datasets).astype(int)\n",
    "offset = 20000\n",
    "tssizes = (betas+1)*offset\n",
    "print(betas)\n",
    "print(tssizes)\n",
    "\n",
    "n_datasets = tssizes.shape[0]\n",
    "\n",
    "# save models to pick best\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b1f012dc5c4f2d9069eaa345decb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Mestrado\\tese\\codigo\\brain_structure_id\\brain_underlying_structure_identification\\walkthrough\\training.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Mestrado/tese/codigo/brain_structure_id/brain_underlying_structure_identification/walkthrough/training.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m generate_timeseries(A,tssizes[i],x0,noise)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Mestrado/tese/codigo/brain_structure_id/brain_underlying_structure_identification/walkthrough/training.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Mestrado/tese/codigo/brain_structure_id/brain_underlying_structure_identification/walkthrough/training.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m features_scaled \u001b[39m=\u001b[39m get_inverted_features(x, \u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Mestrado/tese/codigo/brain_structure_id/brain_underlying_structure_identification/walkthrough/training.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Mestrado/tese/codigo/brain_structure_id/brain_underlying_structure_identification/walkthrough/training.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m features_ss \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(features_scaled)\n",
      "File \u001b[1;32mc:\\Mestrado\\tese\\codigo\\brain_structure_id\\brain_underlying_structure_identification\\walkthrough\\helper_functions.py:512\u001b[0m, in \u001b[0;36mget_inverted_features\u001b[1;34m(timeseries, n, undirected)\u001b[0m\n\u001b[0;32m    510\u001b[0m z1 \u001b[39m=\u001b[39m z[:,offset\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:tsize]\n\u001b[0;32m    511\u001b[0m z2 \u001b[39m=\u001b[39m z[:,\u001b[39m1\u001b[39m:tsize\u001b[39m-\u001b[39moffset]\n\u001b[1;32m--> 512\u001b[0m R \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(z1,z2\u001b[39m.\u001b[39;49mT)\u001b[39m/\u001b[39m(tsize\u001b[39m-\u001b[39moffset)\n\u001b[0;32m    513\u001b[0m r_inv \u001b[39m=\u001b[39m func(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv(R))\n\u001b[0;32m    514\u001b[0m r \u001b[39m=\u001b[39m func(R)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate over number of runs\n",
    "for run in (t := trange(n_runs)):\n",
    "\n",
    "    # concatenate datasets for training \n",
    "    for i in range(n_datasets):\n",
    "\n",
    "        # random graph size (number of nodes)\n",
    "        sz = np.random.randint(30, 80) \n",
    "\n",
    "        # generate the adjacency and A matrices\n",
    "        adj = get_adjacency(sz,p,undirected)\n",
    "        A = get_A(adj,c,rho)\n",
    "        y = get_target(A)\n",
    "\n",
    "        # generate noise \n",
    "        noise = generate_noise(sz, tssizes[i], alpha, betas[i])\n",
    "\n",
    "        # generates the synthetic time series\n",
    "        x = generate_timeseries(A,tssizes[i],x0,noise)\n",
    "\n",
    "        # features\n",
    "        features_scaled = get_inverted_features(x, 100)\n",
    "        scaler = StandardScaler()\n",
    "        features_ss = scaler.fit_transform(features_scaled)\n",
    "\n",
    "        if i == 0:\n",
    "            ys = y\n",
    "            features_ss2 = features_ss\n",
    "\n",
    "        else:\n",
    "            ys = np.concatenate((ys,y),axis=0)\n",
    "            features_ss2 = np.concatenate((features_ss2, features_ss), axis=0)\n",
    "\n",
    "    # shuffle datasets\n",
    "    n_samples = features_ss2.shape[0]\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    features_ss2 = features_ss2[perm,:]\n",
    "    ys = ys[perm,:]\n",
    "\n",
    "    # train\n",
    "    model = train_model_fnn(features_ss2, ys, features_ss2.shape[1], log_name=f'_s_{run}')\n",
    "\n",
    "    # test\n",
    "    idd = y_test < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    idc = y_test > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    # calculate accuracy and idgap on test set\n",
    "    pred = model.predict(X_test, verbose=0)\n",
    "    dis_mpred = model.predict(X_test[idd,:], verbose=0)\n",
    "    con_mpred = model.predict(X_test[idc,:], verbose=0)\n",
    "    acc, idgap = get_acc_idgap(pred, y_test, dis_mpred, con_mpred)\n",
    "\n",
    "    # save model and performance\n",
    "    models.append((model, acc, idgap))\n",
    "\n",
    "    t.set_description(f'acc {acc} idgap {idgap}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_models(models):\n",
    "    \"\"\"\n",
    "        sort models by accuracy\n",
    "        in tiebreak:\n",
    "            sort models by the maximum of idgap\n",
    "    \"\"\"\n",
    "    return sorted(models, key=lambda x: ( -x[1], abs(0.5 - x[2]) ) )\n",
    "\n",
    "file_name = \"model_ffnn\"\n",
    "\n",
    "# sort models based on accuracy and idgap\n",
    "models = sort_models(models)\n",
    "_model = models[0]\n",
    "model = _model[0]\n",
    "print(_model)\n",
    "print(models)\n",
    "\n",
    "save_model(model,file_name+'_ss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
