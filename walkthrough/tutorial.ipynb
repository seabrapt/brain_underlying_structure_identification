{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial shows the generation process of the synthetic data and one experiment, as an example of the setup created to study the performance of the proprosed module. Some of the functions used in this file are defined in the helper_functions.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Packages (pip freeze) in requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import helper functions to generate graphs and timeseries\n",
    "from helper_functions import *\n",
    "\n",
    "# keras\n",
    "from keras.models import load_model\n",
    "\n",
    "# z normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# misc\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the model, structure and layers, used in the classification phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model and viewing the architecture\n",
    "model = get_model_architecture(200)\n",
    "model.load_weights(\"./weights/directed.h5\")\n",
    "#model.load_weights(\"./weights/undirected.h5\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = get_cnn_architecture(200)\n",
    "cnn.load_weights(\"./weights/cnn.h5\")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated synthetic data has some variables that need to be defined in order to build the underlying graph (first code cell) and to compose the  time-series (second code cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the adjacency matrix\n",
    "number_of_nodes = 5     # number of nodes of the graph\n",
    "probability = 0.5       # probability of 2 nodes being connected \n",
    "undirected = True       # if graph is directed or undirected\n",
    "adj = get_adjacency(number_of_nodes, probability, undirected)\n",
    "print(\"Adjacency Matrix\\n\", adj)\n",
    "\n",
    "# generating the interaction matrix\n",
    "c = 0.9                 # parameter of laplacian rule\n",
    "rho = 0.75              # parameter of laplacian rule\n",
    "A = get_A(adj, c, rho)\n",
    "print(\"\\nInteraction Matrix\\n\", A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the noise \n",
    "number_of_samples = 10000       # number of samples to generate\n",
    "alpha = 1                       # intensity of diagonal noise\n",
    "beta = 100                      # intensity of colored noise \n",
    "noise = generate_noise(number_of_nodes, number_of_samples, alpha, beta)\n",
    "print(\"Noise shape: \", noise.shape)\n",
    "\n",
    "# generating timeseries\n",
    "x0 = 0                          # initial value of all nodes \n",
    "timeseries = generate_timeseries(A, number_of_samples, x0, noise)\n",
    "print(\"Timeseries shape: \", timeseries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the engineered features we only need the observable time-series (n_observable - number of observable nodes) and the number of lag-moments included in the feature vector. For example, if we define n_features=200, the feature vector will be composed by the 50 first positive and negative correlation lag matrices and their invertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 200 # number of lag moments\n",
    "n_observable = 4 # number of observable nodes\n",
    "feature_vector = get_inverted_features(timeseries[:,0:n_observable], n_features)\n",
    "print(\"Feature-vector shape (number of node pairs, number of features): \", feature_vector.shape)\n",
    "\n",
    "# z normalization\n",
    "scaler = StandardScaler()\n",
    "feature_vector_ss = scaler.fit_transform(feature_vector)\n",
    "\n",
    "# predict\n",
    "preds = model.predict(feature_vector_ss)\n",
    "\n",
    "# cluster\n",
    "preds = cluster_predictions(preds, 'kmeans')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the state of the art estimators used as references to measure the behaviour of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1 estimator\n",
    "r1 = get_r1_preds(timeseries[:,0:n_observable])\n",
    "print(\"R1 preds:\\t\\t\", r1)\n",
    "\n",
    "# R1 - R3 estimator\n",
    "r1_r3 = get_r1_minus_r3_preds(timeseries[:,0:n_observable])\n",
    "print(\"R1 - R3 preds:\\t\\t\", r1_r3)\n",
    "\n",
    "# Precision Matrix estimator\n",
    "precision_matrix = get_r0_inv_preds(timeseries[:,0:n_observable])\n",
    "print(\"Precision Matrix preds:\\t\", precision_matrix)\n",
    "\n",
    "# Granger\n",
    "granger = get_granger_preds(timeseries[:,0:n_observable])\n",
    "print(\"Granger preds:\\t\\t\", granger)\n",
    "\n",
    "# our method\n",
    "print(\"Our method:\\t\\t\", preds)\n",
    "\n",
    "# Target (Ground truth structure)\n",
    "As = A[0:n_observable,0:n_observable]\n",
    "y = get_target(As)\n",
    "print(\"\\nGround Truth:\\t\\t\", y.squeeze())\n",
    "y = get_target(A)\n",
    "print(\"Ground Truth (all):\\t\", y.squeeze())\n",
    "\n",
    "print(\"\\nNote: Connected pairs: 2\\tDisconnected pairs: 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Runs: comparing estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell goes through some parameters variation to see its influence on the model performance comparing with the state of the art estimators shown before. In order to obtain more precise performance analysis and smooth out the performance variation for each number of samples, the same test is done nruns times. In each run the number of timeseries samples begins with the value defined by the variable min_size and increases in each step a value defined by the variable step, until it reaches the value defined by the variable max_size. The number of observable nodes is defined as an interval[ss,se], specifying the exact observable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "min_size = 10000\n",
    "tsize = 500000\n",
    "max_size= tsize\n",
    "step = 51000\n",
    "tssize = np.arange(min_size,max_size,step=step) \n",
    "print(tssize)\n",
    "\n",
    "# graph parameters\n",
    "sz = 120    # total number of nodes\n",
    "ss = 0      # observable nodes\n",
    "se = 30     # observable nodes\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "p = 0.7\n",
    "undirected = True\n",
    "\n",
    "# Define the range of noise variance\n",
    "qsi = 1\n",
    "beta = 200\n",
    "x0 = 0            #Initial condition\n",
    "\n",
    "\n",
    "# cnn model Â´\n",
    "model_cnn = get_cnn_architecture(n_features=200)\n",
    "model_cnn.load_weights('./weights/cnn.h5')\n",
    "\n",
    "# our approach\n",
    "model = get_model_architecture(n_features=200)\n",
    "model.load_weights('./weights/directed.h5')\n",
    "\n",
    "# number of runs\n",
    "nruns = 3\n",
    "comparison_data = np.zeros((len(tssize), nruns, 6, 2))\n",
    "\n",
    "# run multiple simulations\n",
    "for i in (t :=  trange(nruns)):\n",
    "    # generate an A matrix\n",
    "    adj = get_adjacency(sz, p, undirected)\n",
    "    A = get_A(adj,c,rho)\n",
    "\n",
    "    # generate noise\n",
    "    noise = generate_noise(sz, tsize, qsi, beta) \n",
    "    \n",
    "    # generate the time series\n",
    "    timeseries = generate_timeseries(A,tsize,x0,noise)\n",
    "    obs = se - ss\n",
    "    obs_id = [i for i in range(ss,se)]\n",
    "    print(obs_id, ss, se)\n",
    "    As = A[obs_id,:]\n",
    "    As = As[:,obs_id]\n",
    "    for j in range(len(tssize)):\n",
    "        \n",
    "        t.set_description(f'Samples {tssize[j]}')\n",
    "        data_aux=np.copy(timeseries)\n",
    "        data=data_aux[0:(tssize[j]),obs_id]\n",
    "        \n",
    "\n",
    "        # features ss\n",
    "        features_unscaled, y = extract_cross_correlation_unscaled_features(As,data,se-ss, 200)\n",
    "        features_scaled = normalize_features(features_unscaled, tssize[j])\n",
    "\n",
    "        # features max\n",
    "        features_max_cnn = features_unscaled / np.max(features_unscaled)\n",
    "        features_max_cnn = features_max_cnn.reshape(features_max_cnn.shape[0],features_max_cnn.shape[1],1)\n",
    "        cnn_preds = model_cnn.predict(features_max_cnn, verbose=0)\n",
    "\n",
    "\n",
    "        # - - - Our model\n",
    "        feature_vector = get_inverted_features(data, 200)\n",
    "        scaler = StandardScaler()\n",
    "        features_ss = scaler.fit_transform(feature_vector)\n",
    "        preds = model.predict(features_ss, verbose=0)\n",
    "        preds = cluster_predictions(preds, method='kmeans')\n",
    "        comparison_data[j, i, 0, 0] = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 0, 1] = balanced_accuracy_score(y,preds)\n",
    "\n",
    "\n",
    "        # - - - granger \n",
    "        preds = get_granger_preds(data, method='gmm')\n",
    "        acc = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 1, 0] = acc\n",
    "        comparison_data[j, i, 1, 1] = balanced_accuracy_score(y,preds)\n",
    "\n",
    "        # - - - CNN Max\n",
    "        preds = cluster_predictions(cnn_preds, method='kmeans')\n",
    "        acc = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 2, 0] = acc\n",
    "        comparison_data[j, i, 2, 1] = balanced_accuracy_score(y,preds)\n",
    "\n",
    "        # - - - Precision matrix\n",
    "        preds = get_r0_inv_preds(data, method='gmm')\n",
    "        acc = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 3, 0] = acc\n",
    "        comparison_data[j, i, 3, 1] = balanced_accuracy_score(y,preds)\n",
    "\n",
    "        # - - - R1\n",
    "        preds = get_r1_preds(data, method='gmm')\n",
    "        acc = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 4, 0] = acc\n",
    "        comparison_data[j, i, 4, 1] = balanced_accuracy_score(y,preds)\n",
    "\n",
    "        # - - - R1 - R3\n",
    "        preds = get_r1_minus_r3_preds(data, method='gmm')\n",
    "        acc = accuracy_score(y,preds)\n",
    "        comparison_data[j, i, 5, 0] = acc\n",
    "        comparison_data[j, i, 5, 1] = balanced_accuracy_score(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    save_name = f\"comparison_data.pkl\"\n",
    "    save_pickle(save_name, comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot average over all runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we plot the data that resulted from the experiment explained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f\"comparison_data_random_idx.pkl\"\n",
    "data = load_pickles(save_name)\n",
    "\n",
    "# plotting line styles    \n",
    "linestyle_tuple = [\n",
    "     ('dotted',                (0, (1, 1))),\n",
    "     ('densely dashed',        (0, (5, 1))),]\n",
    "\n",
    "# average over runs\n",
    "data_plot = data.mean(axis=1)\n",
    "data_std= data.std(axis=1)\n",
    "\n",
    "print(data_plot.shape)\n",
    "\n",
    "# metric to plot\n",
    "metric = 1\n",
    "metrics = [\"Accuracy\", \"Accuracy balanced\"]\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.title(f'Probability {p}% | Nodes {sz} Observables {se}\\nBeta: {beta}')\n",
    "plt.plot(tssize, data_plot[:,0,metric], color='k', linewidth=2.5, label='FFNN with $\\mathcal{K}(n)$ (our method)')\n",
    "plt.plot(tssize, data_plot[:,1,metric], color='orange', linewidth=2.5, label='Granger', ls='--')\n",
    "plt.plot(tssize, data_plot[:,2,metric], color='green', linewidth=2.5, label='CNN with $\\mathcal{F}(n)$ [11]', ls='-.')\n",
    "plt.plot(tssize, data_plot[:,3,metric], color='red', linewidth=2.5, label='Precision Matrix', ls=':')\n",
    "plt.plot(tssize, data_plot[:,4,metric], color='purple', linewidth=2.5, label='$R_1$', ls=linestyle_tuple[0][1])\n",
    "plt.plot(tssize, data_plot[:,5,metric], color='brown', linewidth=2.5, label='$R_1$-$R_3$ (NIG)', ls=linestyle_tuple[1][1])\n",
    "\n",
    "\n",
    "# variation plot\n",
    "step = 1\n",
    "plt.errorbar(tssize[::step], data_plot[::step,0,metric], data_std[::step,0,metric], color='k', linewidth=1, ls='None')\n",
    "plt.errorbar(tssize[::step], data_plot[::step,1,metric], data_std[::step,1,metric],color='orange', linewidth=1, ls='None')\n",
    "plt.errorbar(tssize[::step], data_plot[::step,2,metric], data_std[::step,2,metric],color='green',linewidth=1, ls='None')\n",
    "plt.errorbar(tssize[::step], data_plot[::step,3,metric], data_std[::step,3,metric],color='red',linewidth=1,  ls='None')\n",
    "plt.errorbar(tssize[::step], data_plot[::step,4,metric], data_std[::step,4,metric],color='purple',linewidth=1, ls='None')\n",
    "plt.errorbar(tssize[::step], data_plot[::step,5,metric], data_std[::step,5,metric],color='brown',linewidth=1, ls='None')\n",
    "\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel(metrics[metric])  \n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(loc='best', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
