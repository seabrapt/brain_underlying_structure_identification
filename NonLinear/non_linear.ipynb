{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to restore variable 'history_100_125', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_25', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_375', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_50', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_625', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_75', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_100_875', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_125', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_25', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_375', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_50', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_625', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_75', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_150_875', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_125', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_25', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_375', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_50', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_625', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_75', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_250_875', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_125', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_25', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_375', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_50', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_625', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_75', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n",
      "Unable to restore variable 'history_50_875', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "#Required packages to run the code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import geopandas\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.manifold import TSNE\n",
    "%store -r\n",
    "%matplotlib tk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN model\n",
    "def save_model_cnn(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model_cnn(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "def save_model_svm(name,model):\n",
    "    pickle.dump(model, open(name, 'wb'))\n",
    "    \n",
    "def load_model_svm(name):\n",
    "    return pickle.load(open(name, 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen A and Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency(sz,p,undirected):\n",
    "    '''\n",
    "    Generates a realization of an Erdős–Rényi Graph Model, undirected or directed.\n",
    "    -First generates of matrix of random floating point numbers in the range [0.0, 1.0].\n",
    "    -If those values are <=p then there is no edge between pairs\n",
    "    -Makes the matrix symmetric if the graoh is undirected\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "\n",
    "        Returns:\n",
    "                adj (2darray): Adjacency matrix\n",
    "    '''\n",
    "    adj = np.random.random((sz, sz)) <= p\n",
    "    adj = np.triu(adj.astype(int))\n",
    "    np.fill_diagonal(adj,0)\n",
    "    if(undirected):\n",
    "        adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "def get_A(adj,c,rho):\n",
    "    '''\n",
    "    Generates the connectivity matrix (interaction weights) from the adjacency matrix according to the laplacian rule\n",
    "\n",
    "        Parameters:\n",
    "                adj (2darray): Adjacency matrix\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1\n",
    "\n",
    "        Returns:\n",
    "                A (2darray): Connectivity matrix\n",
    "    '''    \n",
    "    sz = len(adj)\n",
    "    Dvec = np.sum(adj, axis=1)\n",
    "    Dmax = np.max(Dvec)\n",
    "    ccc = c*1/Dmax\n",
    "    D = np.diag(Dvec)\n",
    "    L = D - adj\n",
    "    Ap = np.eye(sz) - ccc*L\n",
    "    A = rho * Ap\n",
    "    return A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(N, n_samples, alpha, beta):\n",
    "    '''\n",
    "        y(n + 1) = alpha * x1(n+1) + beta * 1*1.T * X2(n+1)\n",
    "\n",
    "        Parameters:\n",
    "                N (int): number of nodes\n",
    "                n_samples (int): number of samples\n",
    "                alpha (float): Standard Deviation of noise X1\n",
    "                Beta (float): Standard Deviation of noise X2\n",
    "\n",
    "        Returns:\n",
    "                z (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    \n",
    "    ones = np.ones((N,N)) * beta/np.sqrt(N)\n",
    "    \n",
    "    z = np.zeros((n_samples, N))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        x1 = np.random.normal(size=(1,N))\n",
    "        x2 = np.random.normal(size=(1,N))\n",
    "        \n",
    "        z[i,:] = alpha * x1 + np.matmul(x2,ones)\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_fun(x):\n",
    "    '''\n",
    "    Transforms the given input according to the following function\n",
    "            \n",
    "            Parameters:\n",
    "                x (int): Value to be transformed\n",
    "                \n",
    "            Returns:\n",
    "                x (int): Transformed x value\n",
    "    ''' \n",
    "    return np.arctan(x)\n",
    "\n",
    "def gen_time_series_non_linear(A,tsize,x0,noise):\n",
    "    '''\n",
    "    Generates the syntetic time series data given the connectivity matrix and the initial condiction x(0), \n",
    "    according to the dynnamical rule y(n + 1) = Ay(n) + x(n + 1)\n",
    "\n",
    "        Parameters:\n",
    "                A (2darray): Connectivity matrix\n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                x (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    sz = len(A)\n",
    "    x = np.zeros((tsize,sz))\n",
    "    x[0,:] = np.ones((1,sz))*x0\n",
    "    for i in range(1,tsize):\n",
    "        for j in range(sz):\n",
    "            nxt = np.dot(A[j,:],x[i-1,:]) + noise[i,:]\n",
    "            x[i,:] = nonlinear_fun(nxt)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rs(time_series,k):\n",
    "    '''\n",
    "    Compute k Features for the nodes 1 and 2\n",
    "\n",
    "        Parameters:\n",
    "            t_1 (2darray): time series of node 1\n",
    "            t_2 (2darray): time series of node 2\n",
    "            k (int): number of features (power of y(m+1))\n",
    "        \n",
    "        Returns:\n",
    "            r1s (3darray): new R1's for non linear dynamical systems\n",
    "            r0s (2darray): R0's of the non linear dynamical system\n",
    "    '''\n",
    "\n",
    "    sz1,sz2 = time_series.shape\n",
    "\n",
    "    r1s = np.zeros((k,sz2,sz2))\n",
    "\n",
    "    aux1 = time_series[0:(sz1-1),:]\n",
    "    aux2 = time_series[1:(sz1),:]\n",
    "\n",
    "    r0s = np.matmul(time_series.T,time_series)\n",
    "    r0s_inverted = np.linalg.inv(r0s)\n",
    "\n",
    "    for i in range(k):\n",
    "        aux = np.matmul(np.power(aux2.T,i),aux1)\n",
    "\n",
    "        r1s[i,:,:] = aux\n",
    "\n",
    "    return r0s_inverted,r1s\n",
    "\n",
    "def create_dataset(sz,tsize,undirected,A,time_series,n_features):\n",
    "    '''\n",
    "    Generates the synthectic data, extracts the features and returns the tranning/testing dataset\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1  \n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "                A (2darray): Grown-Truth matrix A\n",
    "\n",
    "        Returns:\n",
    "                data (2darray): Matrix containing the feature-vectors between each pair of nodes\n",
    "                target (1darray): Ground-truth - pairs are connected or disconnected\n",
    "    '''\n",
    "\n",
    "    #Is the graph undirected or directed\n",
    "    if(undirected):\n",
    "        \n",
    "        #Create data structures\n",
    "        upper = int(sz*(sz-1)/2)  #Number of elements in the upper matrix\n",
    "        data = np.zeros((n_features,upper))\n",
    "        target = np.zeros((1,upper))\n",
    "        \n",
    "        #Goes through each pair (of the upper matrix) and computes the non linear time laged matrix (excludes diagonal)\n",
    "        counter = 0\n",
    "\n",
    "        r0s_inverted,r1s = get_rs(time_series,n_features)\n",
    "\n",
    "        for i in range(sz):\n",
    "            for j in range(i+1,sz):\n",
    "                #Extracts the first negative and positive lags\n",
    "                data[:,counter] = r1s[:,i,j] * r0s_inverted[i,j]\n",
    "                #Saves the data\n",
    "                target[0,counter] = A[i,j]\n",
    "                counter += 1\n",
    "    return data,target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,y_train,n_features):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(n_features,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, callbacks=[cb], verbose = 0)\n",
    "    return model\n",
    "\n",
    "def train_cnn_new_model(X_train,y_train,n_features):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=20)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(n_features, 1)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=200, validation_split=0.1, callbacks=[cb], verbose = 0)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Gen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data' (ndarray)\n",
      "Stored 'y' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "sz = 40     #Number of nodes\n",
    "p = 0.50  #Probability of nodes being connected (Erdős–Rényi)\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "#Define the range of noise variance\n",
    "alpha = 0.1\n",
    "beta = 0\n",
    "tsize = 1000000    #Number of samples (time series size)\n",
    "x0 = 0            #Initial condition\n",
    "k = 500\n",
    "\n",
    "undirected = True #graph undirected or not\n",
    "\n",
    "adj = get_adjacency(sz,p,undirected)\n",
    "\n",
    "A = get_A(adj,c,rho)\n",
    "\n",
    "noise = generate_noise(sz,tsize,alpha,beta)\n",
    "\n",
    "#get time series with diagonal noise\n",
    "time_series = gen_time_series_non_linear(A,tsize,x0,noise)\n",
    "\n",
    "data,target = create_dataset(sz,tsize,undirected,A,time_series,k)\n",
    "\n",
    "y = target>0\n",
    "y = y.astype(int)\n",
    "\n",
    "%store data\n",
    "%store y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn_4 = data[147:151,:]\n",
    "\n",
    "data_cnn_10 = data[144:154,:]\n",
    "\n",
    "data_cnn_24 = data[137:161,:]\n",
    "\n",
    "data_cnn_30 = data[134:164,:]\n",
    "\n",
    "data_cnn_40 = data[129:169,:]\n",
    "\n",
    "data_cnn_50 = data[224:274,:]\n",
    "\n",
    "data_cnn_60 = data[119:179,:]\n",
    "\n",
    "data_cnn_70 = data[114:184,:]\n",
    "\n",
    "data_cnn_80 = data[109:189,:]\n",
    "\n",
    "data_cnn_90 = data[104:194,:]\n",
    "\n",
    "data_cnn_100 = data[199:299,:]\n",
    "\n",
    "data_cnn_150 = data[174:324,:]\n",
    "\n",
    "data_cnn_200 = data[149:349,:]\n",
    "\n",
    "data_cnn_250 = data[124:374,:]\n",
    "\n",
    "data_cnn_300 = data[99:399,:]\n",
    "\n",
    "data_cnn_350 = data[74:424,:]\n",
    "\n",
    "data_cnn_400 = data[49:449,:]\n",
    "\n",
    "data_cnn_450 = data[24:474,:]\n",
    "\n",
    "data_cnn_500 = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seabr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\seabr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2521f8802b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_c = y[0] > 0\n",
    "idx_d = y[0] < 1\n",
    "\n",
    "X_tsne_c = TSNE(learning_rate=100).fit_transform(data.T[idx_c,:])\n",
    "X_tsne_d = TSNE(learning_rate=100).fit_transform(data.T[idx_d,:])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_tsne_c[:, 0], X_tsne_c[:, 1],c = 'green')\n",
    "plt.scatter(X_tsne_d[:, 0], X_tsne_d[:, 1],c = 'red')\n",
    "plt.title(\"Feature Reduction to 2d using t-SNE\")\n",
    "plt.legend(['Connected','Disconected'], loc = 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI Between Features and A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mutual-Information')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks = np.arange(0,k,1)\n",
    "\n",
    "skb = SelectKBest(mutual_info_classif, k=1).fit(data.T,y.T.flatten())\n",
    "mi = skb.scores_\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ks,mi)\n",
    "plt.title(\"Mutual-Info between A and Non-Linear Features\")\n",
    "plt.xlabel(\"k values\")\n",
    "plt.ylabel(\"Mutual-Information\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n",
      "Run 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_50features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_50features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i+1))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_50.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,50)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_50features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_100features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_100features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_100.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,100)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_100features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 150 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_150features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_150features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_150.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,150)\n",
    "    \n",
    "    models_list.append(model)\n",
    "\n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_150features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_200features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_200features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_200.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,200)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_200features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 250 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_250features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_250features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_250.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,250)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_250features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Run 10\n",
      "Run 11\n",
      "Run 12\n",
      "Run 13\n",
      "Run 14\n",
      "Run 15\n",
      "Run 16\n",
      "Run 17\n",
      "Run 18\n",
      "Run 19\n",
      "Run 20\n",
      "Run 21\n",
      "Run 22\n",
      "Run 23\n",
      "Run 24\n",
      "Run 25\n",
      "Run 26\n",
      "Run 27\n",
      "Run 28\n",
      "Run 29\n",
      "Run 30\n",
      "Run 31\n",
      "Run 32\n",
      "Run 33\n",
      "Run 34\n",
      "Run 35\n",
      "Run 36\n",
      "Run 37\n",
      "Run 38\n",
      "Run 39\n",
      "Run 40\n",
      "Run 41\n",
      "Run 42\n",
      "Run 43\n",
      "Run 44\n",
      "Run 45\n",
      "Run 46\n",
      "Run 47\n",
      "Run 48\n",
      "Run 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_300features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_300features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_300.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,300)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_300features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 350 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_350features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_350features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_350.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,350)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_350features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 400 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_400features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_400features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_400.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,400)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_400features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 450 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_450features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_450features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_450.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,450)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_450features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_500features_0.50p_sergioArchitecture\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_nonlinear_500features_0.50p_sergioArchitecture\\assets\n"
     ]
    }
   ],
   "source": [
    "nruns = 10\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "for i in range(nruns):\n",
    "    print(\"Run \" + str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_cnn_500.T,y.T, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    #Train the model\n",
    "    model = train_model(X_train,y_train,500)\n",
    "    \n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Disconnected samples\n",
    "    idd = y_test < 1\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 0\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:],verbose = 0)\n",
    "    cpred = model.predict(X_test[idc,:,:],verbose = 0)\n",
    "    \n",
    "    threshold = (max(dpred) + min(cpred)) / 2\n",
    "    \n",
    "    trued = np.sum(dpred<threshold)\n",
    "    truec = np.sum(cpred>threshold)\n",
    "    true = trued + truec\n",
    "    acc = true/(len(dpred)+len(cpred))*100\n",
    "    \n",
    "    performance_list.append(acc)\n",
    "    \n",
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "save_model_cnn(best_model,'cnn_nonlinear_500features_0.50p_sergioArchitecture')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% Prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 5000\n",
      "Samples: 10000\n",
      "Samples: 15000\n",
      "Samples: 20000\n",
      "Samples: 25000\n",
      "Samples: 30000\n",
      "Samples: 35000\n",
      "Samples: 40000\n",
      "Samples: 45000\n",
      "Samples: 50000\n",
      "Samples: 55000\n",
      "Samples: 60000\n",
      "Samples: 65000\n",
      "Samples: 70000\n",
      "Samples: 75000\n",
      "Samples: 80000\n",
      "Samples: 85000\n",
      "Samples: 90000\n",
      "Samples: 95000\n",
      "Stored 'models_performance' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "t_samples = 100000\n",
    "step = 5000\n",
    "n_runs = 10\n",
    "\n",
    "sz = 40     #Number of nodes\n",
    "p = 0.50  #Probability of nodes being connected (Erdős–Rényi)\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "undirected = True\n",
    "\n",
    "#Define the range of noise variance\n",
    "alpha = 0.1\n",
    "beta = 0\n",
    "x0 = 0            #Initial condition\n",
    "k = 500\n",
    "\n",
    "x = np.arange(step,t_samples,step)\n",
    "\n",
    "s1 = \"C:/Users/seabr/Desktop/investigação/Code/models/NonLinear/cnn_nonlinear_\"\n",
    "s2 = \"features_0.50p_sergioArchitecture\"\n",
    "\n",
    "models = [\"500\"]\n",
    "models_performance = np.zeros((len(models),len(x)))\n",
    "\n",
    "count = 0\n",
    "for i in models:\n",
    "    models[count] = s1 + i + s2\n",
    "    count +=1\n",
    "\n",
    "\n",
    "count_2 = 0\n",
    "\n",
    "for samples in x:\n",
    "    print(\"Samples: \" + str(samples))\n",
    "    performance_list = np.zeros((len(models),n_runs))\n",
    "\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        adj = get_adjacency(sz,p,undirected)\n",
    "\n",
    "        A = get_A(adj,c,rho)\n",
    "\n",
    "        noise = generate_noise(sz,samples,alpha,beta)\n",
    "\n",
    "        #get time series with diagonal noise\n",
    "        time_series = gen_time_series_non_linear(A,samples,x0,noise)\n",
    "\n",
    "        data,target = create_dataset(sz,samples,undirected,A,time_series,k)\n",
    "\n",
    "        data_cnn_50 = data[224:274,:]\n",
    "        data_cnn_50 = data_cnn_50.T.reshape((data_cnn_50.T.shape[0], data_cnn_50.T.shape[1], 1))\n",
    "        \n",
    "        data_cnn_100 = data[199:299,:]\n",
    "        data_cnn_100 = data_cnn_100.T.reshape((data_cnn_100.T.shape[0], data_cnn_100.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_150 = data[174:324,:]\n",
    "        data_cnn_150 = data_cnn_150.T.reshape((data_cnn_150.T.shape[0], data_cnn_150.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_200 = data[149:349,:]\n",
    "        data_cnn_200 = data_cnn_200.T.reshape((data_cnn_200.T.shape[0], data_cnn_200.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_250 = data[124:374,:]\n",
    "        data_cnn_250 = data_cnn_250.T.reshape((data_cnn_250.T.shape[0], data_cnn_250.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_300 = data[99:399,:]\n",
    "        data_cnn_300 = data_cnn_300.T.reshape((data_cnn_300.T.shape[0], data_cnn_300.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_350 = data[74:424,:]\n",
    "        data_cnn_350 = data_cnn_350.T.reshape((data_cnn_350.T.shape[0], data_cnn_350.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_400 = data[49:449,:]\n",
    "        data_cnn_400 = data_cnn_400.T.reshape((data_cnn_400.T.shape[0], data_cnn_400.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_450 = data[24:474,:]\n",
    "        data_cnn_450 = data_cnn_450.T.reshape((data_cnn_450.T.shape[0], data_cnn_450.T.shape[1], 1))\n",
    "\n",
    "        data_cnn_500 = data\n",
    "        data_cnn_500 = data_cnn_500.reshape((data_cnn_500.T.shape[0], data_cnn_500.T.shape[1], 1))\n",
    "\n",
    "        #datas = [data_cnn_50,data_cnn_100,data_cnn_150,data_cnn_200,data_cnn_250,data_cnn_300,data_cnn_350,data_cnn_400,data_cnn_450,data_cnn_500]\n",
    "        datas = [data_cnn_500]\n",
    "\n",
    "        y = target>0\n",
    "        y = y.astype(int)\n",
    "\n",
    "        #get predicts\n",
    "        count = 0\n",
    "        for m in models:\n",
    "            model = load_model_cnn(m)\n",
    "\n",
    "            pred = model.predict(datas[count],verbose = 0)\n",
    "\n",
    "            y_pred = pred.T > np.mean(pred)\n",
    "            y_pred = y_pred.astype(int)\n",
    "            trues = np.sum(y == y_pred)\n",
    "            acc = (trues/len(y_pred[0]))*100\n",
    "\n",
    "            performance_list[count][i] = acc\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    for j in range(len(models)):\n",
    "        models_performance[j][count_2] = np.mean(performance_list[j])\n",
    "    \n",
    "    count_2 +=1\n",
    "\n",
    "%store models_performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50.25641026 49.30769231 50.07692308 49.21794872 50.6025641  50.3974359\n",
      "  49.64102564 48.98717949 49.67948718 50.15384615 49.64102564 49.96153846\n",
      "  50.28205128 50.84615385 50.43589744 50.61538462 50.12820513 50.15384615\n",
      "  50.03846154]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1dc9ec82e00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_samples = 100000\n",
    "step = 5000\n",
    "x = np.arange(step,t_samples,step)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,models_performance[0])\n",
    "plt.title(\"500 features results on non-linear system\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend(['500 features model'],loc = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
